{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loading_utils\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from functools import partial\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download data\n",
    "# !wget \"https://raw.githubusercontent.com/saprmarks/feature-circuits/main/data/nounpp_train.json\"\n",
    "# !wget \"https://raw.githubusercontent.com/saprmarks/feature-circuits/main/data/nounpp_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "saes = []\n",
    "for i in range(12):\n",
    "    saes.append(SAE.from_pretrained(\n",
    "        release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "        sae_id = f\"blocks.{i}.hook_resid_pre\", # won't always be a hook point\n",
    "        device = device\n",
    "    )[0])  # returns SAE, config, sparsity\n",
    "\n",
    "\n",
    "# Load data\n",
    "train, test = loading_utils.load_examples(\"nounpp_train.json\", 9999, model), loading_utils.load_examples(\"nounpp_test.json\", 9999, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train[0]\n",
    "\n",
    "def attr_patch(model: HookedTransformer, sample, saes, metric, node_thresh=0.1, edge_thresh=0.01, verify=False):\n",
    "    sae_cache = {}\n",
    "    model.reset_hooks()\n",
    "\n",
    "    grad_cache = {}\n",
    "    if verify:\n",
    "        embs_cache = []\n",
    "\n",
    "\n",
    "        def embedding_hook(output, hook):\n",
    "            output.retain_grad()\n",
    "            embs_cache.append(output)\n",
    "            return output\n",
    "        \n",
    "        model.add_hook('hook_embed', embedding_hook, 'fwd')\n",
    "\n",
    "        result_baseline = metric(sample, model(sample['patch_prefix'], return_type='logits'))\n",
    "        result_baseline.backward()\n",
    "        # print('embs:', embs_cache.grad, embs_cache.shape, embs_cache.grad.shape)\n",
    "\n",
    "    def sae_fwd_hook(output, hook, sae):\n",
    "        # res1 (output) -> wide SAE (hook point) -> res2\n",
    "        # our return statement is what the activation at res2 should be\n",
    "        enc = sae.encode(output)\n",
    "        enc.retain_grad()\n",
    "        sae_cache[sae.cfg.hook_name] = enc\n",
    "        dec = sae.decode(enc)\n",
    "        err = (output - dec).detach()\n",
    "        return dec + err\n",
    "\n",
    "\n",
    "    def sae_bwd_hook(grad, hook, sae):\n",
    "        # res2 -> wide SAE (hook point) -> res1 (grad is here), and we skip over wide SAE\n",
    "        # our return statement is what the gradient at res2 should be\n",
    "        # grad_cache[sae.cfg.hook_name] = grad\n",
    "        return (grad,)\n",
    "    \n",
    "    for sae in saes:\n",
    "        model.add_hook(sae.cfg.hook_name, partial(sae_fwd_hook, sae=sae), 'fwd')\n",
    "        model.add_hook(sae.cfg.hook_name, partial(sae_bwd_hook, sae=sae), 'bwd')\n",
    "\n",
    "    clean_result = metric(sample, model(sample['clean_prefix'], return_type='logits'))\n",
    "    clean_result.backward(retain_graph=True)\n",
    "    sae_clean = sae_cache  # retain reference to clean activations\n",
    "    sae_cache = {}  # reset sae_cache to collect patch activations\n",
    "    clean_grads = {k: v.grad.detach() for k,v in sae_clean.items()}\n",
    "    if verify:\n",
    "        print(\"grad diff for hooked model at embedding layer:\", abs(embs_cache[1]-embs_cache[0]).sum().item())\n",
    "    \n",
    "\n",
    "    patch_result = metric(sample, model(sample['patch_prefix'], return_type='logits'))\n",
    "    sae_patch = sae_cache \n",
    "    sae_cache = {}\n",
    "    # input * grad attribution\n",
    "    attribs = {k: (clean_grads[k] * (sae_patch[k] - sae_clean[k])).detach() for k in sae_clean}\n",
    "    attrib_good_indices = {k: (attrib > node_thresh).nonzero() for k, attrib in attribs.items()}\n",
    "\n",
    "    # refill sae_cache with clean run\n",
    "    metric(sample, model(sample['clean_prefix'], return_type='logits'))\n",
    "    sae_clean = sae_cache\n",
    "\n",
    "    model.zero_grad()\n",
    "    layer_pat = 'blocks.{i}.hook_resid_pre'\n",
    "    edge_attribs = {}\n",
    "    edge_attrib_good_indices = {}\n",
    "    for layer in range(11, 0, -1):\n",
    "        down = layer_pat.format(i=layer)\n",
    "        up = layer_pat.format(i=layer-1)\n",
    "        edge_attribs[down] = {}\n",
    "        edge_attrib_good_indices[down] = {}\n",
    "        for feat_tr in attrib_good_indices[down]:\n",
    "            to_backprop = clean_grads[down] * sae_clean[down]\n",
    "            feat = tuple(feat_tr.cpu().numpy())\n",
    "            to_backprop[feat].backward(retain_graph=True)\n",
    "            edge_attribs[down][feat] = (sae_clean[up].grad * (sae_patch[up] - sae_clean[up])).detach()\n",
    "            sae_clean[up].grad.zero_()\n",
    "            edge_attrib_good_indices[down][feat] = (edge_attribs[down][feat] > edge_thresh).nonzero()\n",
    "            model.zero_grad()\n",
    "\n",
    "    model.reset_hooks()    \n",
    "\n",
    "    return attribs, attrib_good_indices, edge_attribs, edge_attrib_good_indices\n",
    "\n",
    "def logit_diff_metric(sample, logits):\n",
    "    last = logits[0, -1]\n",
    "    return last[sample['clean_answer']] - last[sample['patch_answer']]\n",
    "\n",
    "node_attr, node_idxs, edges_attr, edge_idxs = attr_patch(model, sample, saes, logit_diff_metric, edge_thresh=0.01)\n",
    "edge_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_sae_grads = {k: grad.detach().cpu().numpy() for k,grad in sae_attribs.items()}\n",
    "# plot histograms of the gradients, one subplot for all 12 layers in a 4 x 3 grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(15, 15))\n",
    "axs = axs.flatten()\n",
    "for i, (k, grad) in enumerate(np_sae_grads.items()):\n",
    "    perct = np.percentile(grad, 50.0)\n",
    "    axs[i].hist(grad[grad > perct].flatten(), bins=100)\n",
    "    axs[i].set_title(f\"Layer {k}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
