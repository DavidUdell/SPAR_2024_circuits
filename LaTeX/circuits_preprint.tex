\documentclass[10pt]{article}

\usepackage{amsmath,amssymb}
\usepackage{chngcntr}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{tabulary}
\usepackage{tabularx}
\usepackage[table]{xcolor}
\usepackage{microtype}
\usepackage{subfig}
\usepackage[accepted]{icml2022}

\newcommand{\embed}{\mathrm{embed}}
\newcommand{\eot}{$\langle$\textbar endoftext\textbar$\rangle$}

\counterwithin{equation}{section}

\begin{document}

\twocolumn[

\icmltitle{SPAR Report: Circuit Phenomenology Using Sparse Autoencoders}
\date{\today}

\begin{icmlauthorlist}
\icmlauthor{David Udell}{aff}
\icmlauthor{Jackson Kaunismaa}{aff}
\icmlauthor{Hardik Bhatnagar}{aff}
\icmlauthor{Himadri Mandal}{aff}
\end{icmlauthorlist}

\icmlaffiliation{aff}{SPAR, Summer 2024 Cohort}
\icmlcorrespondingauthor{David Udell}{udelldavidb@gmail.com}

\vskip 0.3in

% End twocolumn
]

\printAffiliationsAndNotice{}

\begin{abstract}
Sparse autoencoders provide a means of projecting model activations into a more interpretable vector space. With them, the field of mechanistic interpretability has taken to trying to understand the internals of large language models during training and during inference. In particular, sparse autoencoder dimensions can be naturally composed into circuits--causal directed graphs in which nodes are autoencoder dimensions and edges are causal effects. We looked at the algorithm for recovering these circuits in \citet{marks2024}.
\end{abstract}


\section{Methods}


\section{Results and Discussion}



\section*{Contributions}
\begin{itemize}
\item David Udell:
\item Jackson Kaunismaa:
\item Hardik Bhatnagar:
\item Himadri Mandal:
\end{itemize}

\bibliographystyle{icml2022}
\bibliography{references}

\end{document}
